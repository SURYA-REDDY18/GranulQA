{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from raptor import BaseSummarizationModel, BaseQAModel, BaseEmbeddingModel, RetrievalAugmentationConfig\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from raptor import RetrievalAugmentation \n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 1) Load & quantize Llama‑2‑7b‑chat into 4‑bit NF4\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "_bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"Loading Llama-2-7B-chat in 4-bit …\")\n",
        "_tokenizer = AutoTokenizer.from_pretrained(_MODEL_NAME, use_fast=False)\n",
        "_model     = AutoModelForCausalLM.from_pretrained(\n",
        "    _MODEL_NAME,\n",
        "    quantization_config=_bnb_cfg,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "print(\"Model loaded in 4-bit NF4\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 2) Summarization & QA wrappers\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "class SummarizationModel(BaseSummarizationModel):\n",
        "    def __init__(self):\n",
        "        self.tokenizer = _tokenizer\n",
        "        self._pipe     = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=_model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "    def summarize(self, context: str, max_tokens: int = 150) -> str:\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"Write a concise, information-dense summary of the following:\\n{context}\"}\n",
        "        ]\n",
        "        prompt = self.tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        out = self._pipe(\n",
        "            prompt,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "        )\n",
        "        return out[0][\"generated_text\"][len(prompt):].strip()\n",
        "\n",
        "class QAModel(BaseQAModel):\n",
        "    def __init__(self):\n",
        "        self.tokenizer = _tokenizer\n",
        "        self._pipe     = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=_model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "    def answer_question(self, context: str, question: str) -> str:\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    f\"Context:\\n{context}\\n\\n\"\n",
        "                    f\"Question: {question}\\n\\n\"\n",
        "                    \"Answer as thoroughly as possible:\"\n",
        "                ),\n",
        "            }\n",
        "        ]\n",
        "        prompt = self.tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        out = self._pipe(\n",
        "            prompt,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "        )\n",
        "        return out[0][\"generated_text\"][len(prompt):].strip()\n",
        "\n",
        "class SBertEmbeddingModel(BaseEmbeddingModel):\n",
        "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def create_embedding(self, text: str):\n",
        "        return self.model.encode(text)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 3) Build RetrievalAugmentation\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "RAC = RetrievalAugmentationConfig(\n",
        "    summarization_model=SummarizationModel(),\n",
        "    qa_model=QAModel(),\n",
        "    embedding_model=SBertEmbeddingModel()\n",
        ")\n",
        "RA = RetrievalAugmentation(config=RAC)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 4) Main evaluation loop (with torch.cuda.empty_cache() after each doc)\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "def main():\n",
        "    ds    = load_dataset(\"deepmind/narrativeqa\", \"default\")\n",
        "    split = ds[\"validation\"].select(range(10))\n",
        "\n",
        "    preds, refs = [], []\n",
        "\n",
        "    for i, ex in enumerate(split):\n",
        "        # unpack document → story string\n",
        "        doc_dict = ex[\"document\"]\n",
        "        doc_text = doc_dict[\"summary\"][\"text\"]\n",
        "\n",
        "        # unpack question → text\n",
        "        q_dict       = ex[\"question\"]\n",
        "        question_txt = q_dict.get(\"text\") or str(q_dict)\n",
        "\n",
        "        # unpack answers → list of strings\n",
        "        answers   = ex[\"answers\"][0]\n",
        "        ref_texts = answers[\"text\"]\n",
        "\n",
        "        # add & answer\n",
        "        RA.add_documents(doc_text)\n",
        "        pred = RA.answer_question(question=question_txt)\n",
        "\n",
        "        preds.append(pred)\n",
        "        refs.append(ref_texts)\n",
        "        print(f\"... processed {i+1}/{len(split)}\")\n",
        "\n",
        "        # free any leftover GPU memory before next iteration\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # compute metrics\n",
        "    bleu   = evaluate.load(\"bleu\")\n",
        "    rouge  = evaluate.load(\"rouge\")\n",
        "    meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "    b1 = bleu.compute(predictions=preds, references=refs, max_order=1)[\"bleu\"]\n",
        "    b4 = bleu.compute(predictions=preds, references=refs)[\"bleu\"]\n",
        "    rL = rouge.compute(predictions=preds, references=refs, use_stemmer=True)[\"rougeL\"]\n",
        "    m  = meteor.compute(predictions=preds, references=refs)[\"meteor\"]\n",
        "\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(f\"BLEU-1:  {b1*100:.2f}\")\n",
        "    print(f\"BLEU-4:  {b4*100:.2f}\")\n",
        "    print(f\"ROUGE-L: {rL*100:.2f}\")\n",
        "    print(f\"METEOR:  {m*100:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/raptor_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n2025-04-21 02:46:42,101 - Loading faiss with AVX2 support.\n2025-04-21 02:46:42,115 - Successfully loaded faiss with AVX2 support.\n2025-04-21 02:46:43,411 - PyTorch version 2.4.1 available.\n2025-04-21 02:46:44,704 - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\nLoading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]\n2025-04-21 02:46:52,953 - Load pretrained SentenceTransformer: sentence-transformers/multi-qa-mpnet-base-cos-v1\n2025-04-21 02:46:53,505 - Use pytorch device_name: cuda\n2025-04-21 02:46:53,860 - Successfully initialized TreeBuilder with Config {'tokenizer': <Encoding 'cl100k_base'>, 'max_tokens': 100, 'num_layers': 5, 'threshold': 0.5, 'top_k': 5, 'selection_mode': 'top_k', 'summarization_length': 100, 'summarization_model': <__main__.SummarizationModel object at 0x7fa307a44250>, 'embedding_models': {'EMB': <__main__.SBertEmbeddingModel object at 0x7fa307cb7af0>}, 'cluster_embedding_model': 'EMB', 'reduction_dimension': 10, 'clustering_algorithm': <class 'raptor.cluster_utils.RAPTOR_Clustering'>, 'clustering_params': {}}\n        Reduction Dimension: 10\n        Clustering Algorithm: RAPTOR_Clustering\n        Clustering Parameters: {}\n        \n2025-04-21 02:46:53,860 - Successfully initialized ClusterTreeBuilder with Config {'tokenizer': <Encoding 'cl100k_base'>, 'max_tokens': 100, 'num_layers': 5, 'threshold': 0.5, 'top_k': 5, 'selection_mode': 'top_k', 'summarization_length': 100, 'summarization_model': <__main__.SummarizationModel object at 0x7fa307a44250>, 'embedding_models': {'EMB': <__main__.SBertEmbeddingModel object at 0x7fa307cb7af0>}, 'cluster_embedding_model': 'EMB', 'reduction_dimension': 10, 'clustering_algorithm': <class 'raptor.cluster_utils.RAPTOR_Clustering'>, 'clustering_params': {}}\n        Reduction Dimension: 10\n        Clustering Algorithm: RAPTOR_Clustering\n        Clustering Parameters: {}\n        \n2025-04-21 02:46:53,861 - Successfully initialized RetrievalAugmentation with Config \n        RetrievalAugmentationConfig:\n            {'tokenizer': <Encoding 'cl100k_base'>, 'max_tokens': 100, 'num_layers': 5, 'threshold': 0.5, 'top_k': 5, 'selection_mode': 'top_k', 'summarization_length': 100, 'summarization_model': <__main__.SummarizationModel object at 0x7fa307a44250>, 'embedding_models': {'EMB': <__main__.SBertEmbeddingModel object at 0x7fa307cb7af0>}, 'cluster_embedding_model': 'EMB', 'reduction_dimension': 10, 'clustering_algorithm': <class 'raptor.cluster_utils.RAPTOR_Clustering'>, 'clustering_params': {}}\n        Reduction Dimension: 10\n        Clustering Algorithm: RAPTOR_Clustering\n        Clustering Parameters: {}\n        \n            \n            \n        TreeRetrieverConfig:\n            Tokenizer: <Encoding 'cl100k_base'>\n            Threshold: 0.5\n            Top K: 5\n            Selection Mode: top_k\n            Context Embedding Model: EMB\n            Embedding Model: <__main__.SBertEmbeddingModel object at 0x7fa307cb7af0>\n            Num Layers: None\n            Start Layer: None\n        \n            \n            QA Model: <__main__.QAModel object at 0x7fa307966e80>\n            Tree Builder Type: cluster\n        \n2025-04-21 02:46:54,633 - Creating Leaf Nodes\nBatches:   0%|          | 0/1 [00:00<?, ?it/s]\nBatches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n\nBatches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n\n\nBatches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n\nBatches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nBatches: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\u001b[A\u001b[A\u001b[A\nBatches: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\u001b[A\nBatches: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n\n\nBatches: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\u001b[A\u001b[A\nBatches: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n\n\n\nBatches: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\nBatches: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n2025-04-21 02:46:55,322 - Created 5 Leaf Embeddings\n2025-04-21 02:46:55,322 - Using Cluster TreeBuilder\n2025-04-21 02:46:55,323 - Constructing Layer 0\n2025-04-21 02:46:55,324 - Stopping Layer construction: Cannot Create More Layers. Total Layers in tree: 0\n2025-04-21 02:46:55,324 - Successfully initialized TreeRetriever with Config \n        TreeRetrieverConfig:\n            Tokenizer: <Encoding 'cl100k_base'>\n            Threshold: 0.5\n            Top K: 5\n            Selection Mode: top_k\n            Context Embedding Model: EMB\n            Embedding Model: <__main__.SBertEmbeddingModel object at 0x7fa307cb7af0>\n            Num Layers: None\n            Start Layer: None\n        \n2025-04-21 02:46:55,324 - Using collapsed_tree\nBatches: 100%|██████████| 1/1 [00:00<00:00, 33.28it/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loading Llama-2-7B-chat in 4-bit …\nModel loaded in 4-bit NF4\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 169\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMETEOR:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[2], line 143\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# add & answer\u001b[39;00m\n\u001b[1;32m    142\u001b[0m RA\u001b[38;5;241m.\u001b[39madd_documents(doc_text)\n\u001b[0;32m--> 143\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mRA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion_txt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m preds\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[1;32m    146\u001b[0m refs\u001b[38;5;241m.\u001b[39mappend(ref_texts)\n",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/midcostt4/code/Users/teja.nagubandi/raptor-master/raptor/RetrievalAugmentation.py:308\u001b[0m, in \u001b[0;36mRetrievalAugmentation.answer_question\u001b[0;34m(self, question, top_k, start_layer, num_layers, max_tokens, collapse_tree, return_layer_information)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# if return_layer_information:\u001b[39;00m\n\u001b[1;32m    304\u001b[0m context, layer_information \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(\n\u001b[1;32m    305\u001b[0m     question, start_layer, num_layers, top_k, max_tokens, collapse_tree, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    306\u001b[0m )\n\u001b[0;32m--> 308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqa_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_layer_information:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer, layer_information\n",
            "Cell \u001b[0;32mIn[2], line 93\u001b[0m, in \u001b[0;36mQAModel.answer_question\u001b[0;34m(self, context, question)\u001b[0m\n\u001b[1;32m     80\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m     {\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     }\n\u001b[1;32m     89\u001b[0m ]\n\u001b[1;32m     90\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     91\u001b[0m     messages, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     92\u001b[0m )\n\u001b[0;32m---> 93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mlen\u001b[39m(prompt):]\u001b[38;5;241m.\u001b[39mstrip()\n",
            "File \u001b[0;32m/anaconda/envs/raptor_env/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:272\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/raptor_env/lib/python3.8/site-packages/transformers/pipelines/base.py:1302\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         )\n\u001b[1;32m   1300\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/raptor_env/lib/python3.8/site-packages/transformers/pipelines/base.py:1309\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1308\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1309\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1310\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m/anaconda/envs/raptor_env/lib/python3.8/site-packages/transformers/pipelines/base.py:1209\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1208\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/raptor_env/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:370\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    368\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 370\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m/anaconda/envs/raptor_env/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/raptor_env/lib/python3.8/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
            "File \u001b[0;32m/anaconda/envs/raptor_env/lib/python3.8/site-packages/transformers/generation/utils.py:3249\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3247\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3248\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3249\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3251\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1745203635804
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "raptor_env",
      "language": "python",
      "display_name": "raptorEnv"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "raptor_env"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
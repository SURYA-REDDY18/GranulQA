{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "912cd8c6-d405-4dfe-8897-46108e6a6af7",
      "metadata": {},
      "source": [
        "# RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "631b09a3",
      "metadata": {
        "gather": {
          "logged": 1745200000656
        }
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'null/Users/teja.nagubandi'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnull/Users/teja.nagubandi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'null/Users/teja.nagubandi'"
          ]
        }
      ],
      "source": [
        "# NOTE: An OpenAI API key must be set here for application initialization, even if not in use.\n",
        "# If you're not utilizing OpenAI models, assign a placeholder string (e.g., \"not_used\").\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d7d995-7beb-40b5-9a44-afd350b7d221",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cinderella story defined in sample.txt\n",
        "with open('demo/sample.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d51ebd-5597-4fdd-8c37-32636395081b",
      "metadata": {},
      "source": [
        "1) **Building**: RAPTOR recursively embeds, clusters, and summarizes chunks of text to construct a tree with varying levels of summarization from the bottom up. You can create a tree from the text in 'sample.txt' using `RA.add_documents(text)`.\n",
        "\n",
        "2) **Querying**: At inference time, the RAPTOR model retrieves information from this tree, integrating data across lengthy documents at different abstraction levels. You can perform queries on the tree with `RA.answer_question`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4f58830-9004-48a4-b50e-61a855511d24",
      "metadata": {},
      "source": [
        "### Building the tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3753fcf9-0a8e-4ab3-bf3a-6be38ef6cd1e",
      "metadata": {
        "gather": {
          "logged": 1745097192736
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/raptor_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2025-04-19 21:13:08,404 - Loading faiss with AVX2 support.\n",
            "2025-04-19 21:13:08,555 - Successfully loaded faiss with AVX2 support.\n"
          ]
        }
      ],
      "source": [
        "from raptor import RetrievalAugmentation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e843edf",
      "metadata": {},
      "outputs": [],
      "source": [
        "RA = RetrievalAugmentation()\n",
        "\n",
        "# construct the tree\n",
        "RA.add_documents(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f219d60a-1f0b-4cee-89eb-2ae026f13e63",
      "metadata": {},
      "source": [
        "### Querying from the tree\n",
        "\n",
        "```python\n",
        "question = # any question\n",
        "RA.answer_question(question)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b4037c5-ad5a-424b-80e4-a67b8e00773b",
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"How did Cinderella reach her happy ending ?\"\n",
        "\n",
        "answer = RA.answer_question(question=question)\n",
        "\n",
        "print(\"Answer: \", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5be7e57",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the tree by calling RA.save(\"path/to/save\")\n",
        "SAVE_PATH = \"demo/cinderella\"\n",
        "RA.save(SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e845de9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load back the tree by passing it into RetrievalAugmentation\n",
        "\n",
        "RA = RetrievalAugmentation(tree=SAVE_PATH)\n",
        "\n",
        "answer = RA.answer_question(question=question)\n",
        "print(\"Answer: \", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "277ab6ea-1c79-4ed1-97de-1c2e39d6db2e",
      "metadata": {},
      "source": [
        "## Using other Open Source Models for Summarization/QA/Embeddings\n",
        "\n",
        "If you want to use other models such as Llama or Mistral, you can very easily define your own models and use them with RAPTOR. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f86cbe7e",
      "metadata": {
        "gather": {
          "logged": 1745012977490
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/raptor_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2025-04-18 21:49:34,402 - Loading faiss with AVX2 support.\n",
            "2025-04-18 21:49:34,416 - Successfully loaded faiss with AVX2 support.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from raptor import BaseSummarizationModel, BaseQAModel, BaseEmbeddingModel, RetrievalAugmentationConfig\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from raptor import RetrievalAugmentation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe5cef43",
      "metadata": {
        "gather": {
          "logged": 1745012977681
        }
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Replace 'your_hf_token_here' with your actual token string\n",
        "login(token=\"{your_hf_token_here}\")\n",
        "\n",
        "Model_Name = \"meta-llama/Llama-2-7b-chat-hf\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3a917b5d-396c-44dd-85df-5a3a10cf41e5",
      "metadata": {
        "gather": {
          "logged": 1745012982915
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏳ Loading Llama-2-7B-chat in 4-bit …\n",
            "✅ Model loaded in 4-bit NF4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-18 21:49:38,240 - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.88s/it]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# One‑time 4‑bit quantisation setup.\n",
        "_bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                 # activate 4‑bit quant\n",
        "    bnb_4bit_quant_type=\"nf4\",         # better accuracy\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,    # second‑level quant for even less VRAM\n",
        ")\n",
        "\n",
        "print(\"⏳ Loading Llama-2-7B-chat in 4-bit …\")\n",
        "_tokenizer = AutoTokenizer.from_pretrained(_MODEL_NAME, use_fast=False)\n",
        "_model     = AutoModelForCausalLM.from_pretrained(\n",
        "    _MODEL_NAME,\n",
        "    quantization_config=_bnb_cfg,\n",
        "    device_map=\"auto\",                 # lets Accelerate place layers on GPU/CPU as needed\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "print(\"✅ Model loaded in 4-bit NF4\")\n",
        "\n",
        "# ——————————————————————————————————————————————\n",
        "# Summarisation wrapper\n",
        "# ——————————————————————————————————————————————\n",
        "class SummarizationModel(BaseSummarizationModel):\n",
        "    def __init__(self):\n",
        "        # reuse global objects so we don’t duplicate VRAM\n",
        "        self.tokenizer = _tokenizer\n",
        "        self._pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=_model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            device_map=\"auto\",          # pipeline picks correct device\n",
        "        )\n",
        "\n",
        "    def summarize(self, context: str, max_tokens: int = 150) -> str:\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Write a concise, information-dense summary of the following:\\n{context}\",\n",
        "            }\n",
        "        ]\n",
        "        prompt = self.tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        outputs = self._pipeline(\n",
        "            prompt,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "        )\n",
        "        return outputs[0][\"generated_text\"][len(prompt) :].strip()\n",
        "\n",
        "# ——————————————————————————————————————————————\n",
        "# Question‑answering wrapper\n",
        "# ——————————————————————————————————————————————\n",
        "class QAModel(BaseQAModel):\n",
        "    def __init__(self):\n",
        "        self.tokenizer = _tokenizer\n",
        "        self._pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=_model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "    def answer_question(self, context: str, question: str) -> str:\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    f\"Context:\\n{context}\\n\\n\"\n",
        "                    f\"Question: {question}\\n\\n\"\n",
        "                    \"Answer as thoroughly as possible:\"\n",
        "                ),\n",
        "            }\n",
        "        ]\n",
        "        prompt = self.tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        outputs = self._pipeline(\n",
        "            prompt,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "        )\n",
        "        return outputs[0][\"generated_text\"][len(prompt) :].strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "245b91a5",
      "metadata": {
        "gather": {
          "logged": 1745012983110
        }
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoTokenizer, pipeline\n",
        "# import torch\n",
        "\n",
        "# # You can define your own Summarization model by extending the base Summarization Class. \n",
        "# class SummarizationModel(BaseSummarizationModel):\n",
        "#     def __init__(self, model_name=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
        "#         # Set use_fast=False to avoid AttributeError with apply_chat_template\n",
        "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "#         self.summarization_pipeline = pipeline(\n",
        "#             \"text-generation\",\n",
        "#             model=model_name,\n",
        "#             tokenizer=self.tokenizer,\n",
        "#             model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "#             device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "#             trust_remote_code=True,\n",
        "#         )\n",
        "\n",
        "#     def summarize(self, context, max_tokens=150):\n",
        "#         messages = [\n",
        "#             {\"role\": \"user\", \"content\": f\"Write a summary of the following, including as many key details as possible: {context}:\"}\n",
        "#         ]\n",
        "#         prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "#         outputs = self.summarization_pipeline(\n",
        "#             prompt,\n",
        "#             max_new_tokens=max_tokens,\n",
        "#             do_sample=True,\n",
        "#             temperature=0.7,\n",
        "#             top_k=50,\n",
        "#             top_p=0.95\n",
        "#         )\n",
        "#         summary = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
        "#         return summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a171496d",
      "metadata": {
        "gather": {
          "logged": 1745012983315
        }
      },
      "outputs": [],
      "source": [
        "# class QAModel(BaseQAModel):\n",
        "#     def __init__(self, model_name=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
        "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "#         self.qa_pipeline = pipeline(\n",
        "#             \"text-generation\",\n",
        "#             model=model_name,\n",
        "#             tokenizer=self.tokenizer,\n",
        "#             model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "#             device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "#             trust_remote_code=True,\n",
        "#         )\n",
        "\n",
        "#     def answer_question(self, context, question):\n",
        "#         messages = [\n",
        "#             {\"role\": \"user\", \"content\": f\"Given Context: {context} Give the best full answer amongst the option to question {question}\"}\n",
        "#         ]\n",
        "#         prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "#         outputs = self.qa_pipeline(\n",
        "#             prompt,\n",
        "#             max_new_tokens=256,\n",
        "#             do_sample=True,\n",
        "#             temperature=0.7,\n",
        "#             top_k=50,\n",
        "#             top_p=0.95\n",
        "#         )\n",
        "#         answer = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
        "#         return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "878f7c7b",
      "metadata": {
        "gather": {
          "logged": 1745012983509
        }
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "class SBertEmbeddingModel(BaseEmbeddingModel):\n",
        "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def create_embedding(self, text):\n",
        "        return self.model.encode(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "255791ce",
      "metadata": {
        "gather": {
          "logged": 1745012985875
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-18 21:49:42,367 - Load pretrained SentenceTransformer: sentence-transformers/multi-qa-mpnet-base-cos-v1\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "2025-04-18 21:49:43,060 - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "2025-04-18 21:49:45,455 - Use pytorch device_name: cuda\n"
          ]
        }
      ],
      "source": [
        "RAC = RetrievalAugmentationConfig(summarization_model=SummarizationModel(), qa_model=QAModel(), embedding_model=SBertEmbeddingModel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fee46f1d",
      "metadata": {
        "gather": {
          "logged": 1745012986070
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-18 21:49:45,557 - Successfully initialized TreeBuilder with Config \n",
            "        TreeBuilderConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Max Tokens: 100\n",
            "            Num Layers: 5\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Summarization Length: 100\n",
            "            Summarization Model: <__main__.SummarizationModel object at 0x7fac3869d820>\n",
            "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x7fad39807ee0>}\n",
            "            Cluster Embedding Model: EMB\n",
            "        \n",
            "        Reduction Dimension: 10\n",
            "        Clustering Algorithm: RAPTOR_Clustering\n",
            "        Clustering Parameters: {}\n",
            "        \n",
            "2025-04-18 21:49:45,558 - Successfully initialized ClusterTreeBuilder with Config \n",
            "        TreeBuilderConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Max Tokens: 100\n",
            "            Num Layers: 5\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Summarization Length: 100\n",
            "            Summarization Model: <__main__.SummarizationModel object at 0x7fac3869d820>\n",
            "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x7fad39807ee0>}\n",
            "            Cluster Embedding Model: EMB\n",
            "        \n",
            "        Reduction Dimension: 10\n",
            "        Clustering Algorithm: RAPTOR_Clustering\n",
            "        Clustering Parameters: {}\n",
            "        \n",
            "2025-04-18 21:49:45,559 - Successfully initialized RetrievalAugmentation with Config \n",
            "        RetrievalAugmentationConfig:\n",
            "            \n",
            "        TreeBuilderConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Max Tokens: 100\n",
            "            Num Layers: 5\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Summarization Length: 100\n",
            "            Summarization Model: <__main__.SummarizationModel object at 0x7fac3869d820>\n",
            "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x7fad39807ee0>}\n",
            "            Cluster Embedding Model: EMB\n",
            "        \n",
            "        Reduction Dimension: 10\n",
            "        Clustering Algorithm: RAPTOR_Clustering\n",
            "        Clustering Parameters: {}\n",
            "        \n",
            "            \n",
            "            \n",
            "        TreeRetrieverConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Context Embedding Model: EMB\n",
            "            Embedding Model: <__main__.SBertEmbeddingModel object at 0x7fad39807ee0>\n",
            "            Num Layers: None\n",
            "            Start Layer: None\n",
            "        \n",
            "            \n",
            "            QA Model: <__main__.QAModel object at 0x7fac3869d430>\n",
            "            Tree Builder Type: cluster\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "RA = RetrievalAugmentation(config=RAC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "afe05daf",
      "metadata": {
        "gather": {
          "logged": 1745013051215
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-18 21:49:45,615 - Creating Leaf Nodes\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n",
            "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.13it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.26it/s]\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.59it/s]A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.63it/s]\u001b[A\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.31it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.06it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.74it/s]\n",
            "2025-04-18 21:49:47,906 - Created 35 Leaf Embeddings\n",
            "2025-04-18 21:49:47,908 - Building All Nodes\n",
            "2025-04-18 21:49:47,909 - Using Cluster TreeBuilder\n",
            "2025-04-18 21:49:47,910 - Constructing Layer 0\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "2025-04-18 21:49:54,420 - Summarization Length: 100\n",
            "2025-04-18 21:50:03,110 - Node Texts Length: 200, Summarized Text Length: 87\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 115.12it/s]\n",
            "2025-04-18 21:50:15,041 - Node Texts Length: 1057, Summarized Text Length: 87\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 122.26it/s]\n",
            "2025-04-18 21:50:23,430 - Node Texts Length: 407, Summarized Text Length: 84\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.14it/s]\n",
            "2025-04-18 21:50:30,526 - Node Texts Length: 186, Summarized Text Length: 80\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.08it/s]\n",
            "2025-04-18 21:50:41,382 - Node Texts Length: 842, Summarized Text Length: 86\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.96it/s]\n",
            "2025-04-18 21:50:50,897 - Node Texts Length: 589, Summarized Text Length: 88\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 123.64it/s]\n",
            "2025-04-18 21:50:50,910 - Constructing Layer 1\n",
            "2025-04-18 21:50:50,911 - Stopping Layer construction: Cannot Create More Layers. Total Layers in tree: 1\n",
            "2025-04-18 21:50:50,911 - Successfully initialized TreeRetriever with Config \n",
            "        TreeRetrieverConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Context Embedding Model: EMB\n",
            "            Embedding Model: <__main__.SBertEmbeddingModel object at 0x7fad39807ee0>\n",
            "            Num Layers: None\n",
            "            Start Layer: None\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "with open('demo/sample.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "    \n",
        "RA.add_documents(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7eee5847",
      "metadata": {
        "gather": {
          "logged": 1745013073222
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-18 21:50:50,916 - Using collapsed_tree\n",
            "\rBatches:   0%|          | 0/1 [00:00<?, ?it/s]\rBatches: 100%|██████████| 1/1 [00:00<00:00, 18.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer:  Cinderella reached her happy ending through a series of events that involved the help of her Fairy Godmother, a magical transformation, and a chance encounter with the king's son at the ball. Here are the key events that led to her happy ending:\n",
            "\n",
            "1. Cinderella's Fairy Godmother appears: Cinderella's Fairy Godmother appears to her in the garden and offers to help her attend the king's son's ball.\n",
            "2. Transformation: The Fairy Godmother transforms a pumpkin into a beautiful golden carriage, mice into horses, and a rat into a coachman. She also gives Cinderella a beautiful dress and slippers.\n",
            "3. Ball: Cinderella attends the ball with the help of her Fairy Godmother and dances with the king's son.\n",
            "4. Midnight: As the clock strikes midnight, Cinderella must leave the ball before her stepfamily discovers she is not who she seems to be. She leaves behind one of her glass slippers.\n",
            "5. Search for the missing bride: The king's son searches for the girl who left behind one of her\n"
          ]
        }
      ],
      "source": [
        "question = \"How did Cinderella reach her happy ending?\"\n",
        "\n",
        "answer = RA.answer_question(question=question)\n",
        "\n",
        "print(\"Answer: \", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6a108830-9f2a-4c31-b8ca-466c2591cc83",
      "metadata": {
        "gather": {
          "logged": 1745125499913
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/raptor_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2025-04-20 05:00:56,585 - Loading faiss with AVX2 support.\n",
            "2025-04-20 05:00:56,752 - Successfully loaded faiss with AVX2 support.\n",
            "2025-04-20 05:01:02,762 - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [01:56<00:00, 58.17s/it]\n",
            "2025-04-20 05:02:59,419 - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
            "2025-04-20 05:03:00,601 - Use pytorch device_name: cuda\n",
            "2025-04-20 05:03:01,393 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
            "2025-04-20 05:03:01,629 - Use pytorch device_name: cuda\n",
            "2025-04-20 05:03:01,793 - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
            "2025-04-20 05:03:02,000 - Use pytorch device_name: cuda\n",
            "2025-04-20 05:03:02,039 - Successfully initialized TreeBuilder with Config {'tokenizer': LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}, 'max_tokens': 100, 'num_layers': 5, 'threshold': 0.5, 'top_k': 5, 'selection_mode': 'top_k', 'summarization_length': 100, 'embedding_models': {'MINILM': <raptor.qa_pipeline.MiniLMEmbeddingModel object at 0x7f671b4615e0>}, 'cluster_embedding_model': 'MINILM'}\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.01it/s]\n",
            "Batches: 100%|██████████| 3/3 [00:00<00:00, 37.28it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.79it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 245.04it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 286.71it/s]\n",
            "/anaconda/envs/raptor_env/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/anaconda/envs/raptor_env/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 207.98it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 196.46it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 215.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏳ Loading Llama-2-7B-chat in 4-bit …\n",
            "\n",
            "\n",
            "Using the provided contexts, Cinderella reached her happy ending by:\n",
            "\n",
            "Context 1: attending the festival with the help of a fairy godmother, who appeared to her in a dream and gave her a magical makeover.\n",
            "\n",
            "Context 2: persevering through difficult circumstances and staying true to her values.\n",
            "\n",
            "Context 3: attending the festival with her parents and step-sisters, despite her step-mother's refusal to let her go.\n"
          ]
        }
      ],
      "source": [
        "from raptor.qa_pipeline import RAPTORLLM\n",
        "\n",
        "with open('demo/sample.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "    \n",
        "\n",
        "raptor = RAPTORLLM()\n",
        "raptor.index_corpus([text])   # one‑time\n",
        "\n",
        "print(raptor.answer(\"How did Cinderella reach her happy ending?\"))"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "raptor_env"
    },
    "kernelspec": {
      "display_name": "raptorEnv",
      "language": "python",
      "name": "raptor_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
